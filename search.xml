<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Reading Note]]></title>
    <url>%2FReading-Note%2F</url>
    <content type="text"></content>
      <categories>
        <category>Learning Note</category>
      </categories>
      <tags>
        <tag>Life Long Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install and Use Vatic Annotation Tool]]></title>
    <url>%2FInstall-and-Use-Vatic-Annotation-Tool%2F</url>
    <content type="text"><![CDATA[##How to install Vatic in a easy way Vatic is a very convinient online annotation tool for labeling objects of videos. The vatic annotation tool enables users to easily annotate video files, where bounding boxes are automatically interpolated in both size and position. Annotation is performed in a web browser, and annotations are stored in a MySQL database. For this, videos need to be locally published using command line interfacing. Extracting the annotation is also performed using the command line interface. The installation requires a lot of configurations, and it is super painful. The installation steps are shown in Vatic . Instead of installing it step by step manually and encountering lots of bugs, we can easily use other people’s docker environment and run it by VM. ###1. Install docker Step1: undate inner core: sudo apt-get install -y --install-recommends linux-generic-lts-trusty Step2: Use scripts to install the docker curl -sSL https://get.docker.com/ | sh ###2. Install and use Vatic reference: vatic-docker Step1: Create “data” folder in the directory where you want to run. Step2: Create “labels.txt” file in data folder. Put all the object types that you want to label on the first line separated by spaces. Step3: Create “video_in” folder in data folder, and put in the video you want to label. Step4: Run the vatic in docker: sudo docker run -it -p 8111:80 -v $PWD/data:/root/vatic/data npsvisionlab/vatic-docker /bin/bash -C /root/vatic/example.sh 8111 is the port. We can change it when needed. Step5: Annotation Open the browser, type: &lt;Your IP address&gt;:8111/directory (Show ip address: ifconfig) Label the objects in the video, go back to /directory whenever you finish the annotations and click output labels button. Find the output file in the root directory. ###Docker Commands Show all containers: sudo docker ps -a Kill the running container: sudo docker kill &lt;Container id&gt; Remove the container: sudo docker rm &lt;Container id&gt;]]></content>
      <categories>
        <category>Data Sets</category>
      </categories>
      <tags>
        <tag>vatic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Installation in Ubuntu 14.04]]></title>
    <url>%2FCaffe-Installation-in-Ubuntu-14-04%2F</url>
    <content type="text"><![CDATA[###Preinstall ###Install CUDA Install cuda by run fileRestart the computer! Install by deb file ###Install cuDNN Step 1. Download cuDNN from Nvidia cuDNN Step 2. Unzip the package. tar -xzvf &lt;cudnn-tar-file&gt;.tgz Step 3. Copy the *.h header files into cuda. cd ~/Downloads/cuda/include // Copy into the installation path of cuda sudo cp *.h /usr/local/cuda-8.0/include/ Step 4. Copy the library files of cuDNN to the cuda directory cd ~/Downloads/cuda/lib64 sudo cp libcudnn* /usr/local/cuda-8.0/lib64/ Step 5. Set the modify type in the terminal. sudo chmod a+r /usr/local/cuda-8.0/include/cudnn.h /usr/local/cuda-8.0/lib64/libcudnn* ###Install OpenCV Remove previous installed OpenCVsudo rm -rf /usr/local/include/opencv2 /usr/local/include/opencv /usr/include/opencv /usr/include/opencv2 /usr/local/share/opencv /usr/local/share/OpenCV /usr/share/opencv /usr/share/OpenCV /usr/local/bin/opencv* /usr/local/lib/libopencv* sudo apt-get purge libopencv-* python-data python-opencv sudo apt-get –purge remove opencv-doc opencv-data python-opencv InstallFirst install it’s dependencies: sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev Then download the latest stable version of OpenCV from Sourceforge. Now create a temporary directory ~/opencv and extract the zip file you downloaded into it: mkdir ~/opencv; cd ~/opencv unzip &lt;open-cv-file&gt;.zip -d . Finally, we’ll make the file in another directory we create called release: cd opencv-&lt;version&gt; mkdir release; cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D WITH_IPP=OFF -D CMAKE_INSTALL_PREFIX=/usr/local .. make sudo make install TestCompile samples: $ cd ~/opencv-3.1.0/samples $ cmake . $ sudo make -j $(nproc) Run samples: $ cd ~/opencv-3.1.0/samples/cpp $ ./cpp-example-facedetect ~/Lenna.png ###Install Caffe]]></content>
      <categories>
        <category>Installation</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN Example#1 Recognize Object]]></title>
    <url>%2FCNN-Example-1-Recognize-Object%2F</url>
    <content type="text"><![CDATA[This article gives a high level overview for the process of recognizing the object by neural network. Step 1To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases: Step 2Let’s combine our four attempts to guess into one big diagram: Step 3Back propagation, using gradient descent algorithm which takes consider of the Cost Function. Step 4Convolution (Make the result translation invariant) 1. Break the image into overlapping image tiles Why we split the image into tiles? Because: Fully connected NN: 1000 × 1000 × 10^6 = 10^12Locally connected NN: 10 × 10 × 10^6 = 10^8 2. Feed each image tile into a small neural network We’ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally(Share the weights). If something interesting appears in any given tile, we’ll mark that tile as interesting. Why we share the weight? Convolutional N: 10 x 10 x 100= 10k 3. Save the results from each tile into a new array Convolution process: 4. Downsampling Use max pooling method to look at each 2x2 square of the array and keep the biggest number, which means to keep the most interesting bit. 5. Do prediction We can use that small array as input into another neural network, this final neural network will decide if the image is or isn’t a match: TipsWhen solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data. The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize. For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it’s knowledge of sharp edges, the third step might recognize entire birds using it’s knowledge of beaks, etc. Referencehttps://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3 https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721]]></content>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Training MNIST dataset by TensorFlow]]></title>
    <url>%2FTraining-MNIST-dataset-by-TensorFlow%2F</url>
    <content type="text"><![CDATA[In this article, I will introduce MNIST data set and review the process of training the MINIST data set to get the model by using TensorFlow. MNIST Data SetThis database is a large database of handwritten digits that is commonly used for training various image processing systems. This database contains 60,000 training images (mnist.train) and 10,000 testing images (mnist.test) 28x28 pixels in one image, we can use 28x28 = 784 dimensions vector to present this matrix. Mnist.train.xs represents 60000 training images. Mnist.train.ys represents the label of the 60000 image. There’re 10 labels from 0 to 9. Each label is the real number shown in each image. Methodology : Softmax RegressionWe use softmax regression to classify each type of writing. 1. The model of the learning 2. Cost function 3. Training Algorithm Use Gradient Descent algorithm, which is backpropagation algorithm.The backpropagation algorithm looks for the minimum of the error function in weight space using the method of gradient descent. Using TensorFlowStep 1: Initialize &amp; start the model init = tf.initialize_all_variables() sess = tf.Session() sess.run(init) Step 2: Training the model (Optimize the weights) for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) // Stochastic training: Randomly use 100 data to train the model. sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) And the train_step is: train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) Step 3: Evaluate the model correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) //y_ is the correct label accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;)) // The accuracy of the model print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) // Print out the accuracy]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Detection Progress]]></title>
    <url>%2FObject-Detection-Progress%2F</url>
    <content type="text"><![CDATA[Part I Computer Vision Field Leaders：http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649439592&amp;idx=1&amp;sn=fdb687300e4a930fdd08c23d8816bbd8&amp;chksm=82c0d4ecb5b75dfae69dd0a219916ab8533da9a6d02d3c7cfbcc5323579c16033bba7407f2b5&amp;scene=21#wechat_redirect David Marr(1945-1980) : He laid the groundwork of computer vision. &lt;&gt; Three major levels of CV: Express(use math to express the problem), Algorithm(to solve the problem), Implement(can be implemented in CPU, DSP or NN) What is computed in CV: primal sketch, 2 1/2 D sketch, 3D sketch, including texture, 3D vision, motion analysis, surface shape. CV is the “progress” of learning the image, is not the result. The longer you observe the image, the more information you will get. 视觉是受任务驱动的，而任务是时刻在改变之中。 视觉求解不是打一个固定的靶子， 而是打一个运动目标。 King-Sun Fu 傅京孫(1930-1985) Syntactic Pattern Recognition Bottom-up and Top-down Ulf Grenander (1923-2016) Pattern Theory( use math and statistic) Proposed analysis-by-synthesis (let the model to generate a image then tell the different between the generated image and the real-world image, then you will know whether this model is a great model) Part II Time Line1999Scale Invariant Feature Transform(SIFT) (improved in 2004) detector descriptor Based on points describe. Feature Based Descriptor (1995~2010)1. Shape Context 2002 Used in MNIST. 2. HOG 2005 Describe the whole patch. 3. Spin Image 1997-&gt;1999 A descriptor of 3D mesh, used in surface matching. 4. STIP (Space time interest points) 2005; HOF (Histogram of oriented optical flow, 2009); MBH (motion boundary histogram, 2013) Object Recognition 2005~20101. LDA (Latent Dirichlet Allocation) 2003 Unsupervised topic modeling, BoW(bag of visual words) algorithm. 2. SPM (Spatial Pyramid Matching) Use spatial grid to separate the image into patches, then calculate the BoW histogram, then combine them together, thus those encoded vector descriptor will have spatial information. 3. Image Encoding Method based on Bow 2006~2009 Sparse coding, Fisher vector to improve BoW (use image encoding) 4. PMK (pyramid matching kernel) 5. DPM (deformable parts models) 2010 Deep Learning 2010~2015Doesn’t need the structure information of the object, multiple layers. n* (convolution layer + pooling layer) + several fully connected layers 1. OverFeat Step 1: Use slide window to get multi-scales ROI. Classify each region by CNN. Step 2: Use regression model to estimate the location of the object. Use bounding box to box the object. Combine the bounding boxes.]]></content>
      <tags>
        <tag>Object Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Detection Methods]]></title>
    <url>%2FObject-Detection-Methods%2F</url>
    <content type="text"><![CDATA[Part I: Basic Method “Histograms of Oriented Gradients for Human Detection,” N. Dalal and W. Triggs, Proc. IEEE CVPR 2005. Improvement: Detect the boundary of the object as well: Learning to Detect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues,” D.R. Martin, C.C. Fowlkes, and J. Malik, Proc. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004. Part II: Object Detection Method1. DPM : “Object Detection Using Discriminatively Trained Part-based Models,” P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, Proc. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. https://people.eecs.berkeley.edu/~rbg/papers/Object-Detection-with-Discriminatively-Trained-Part-Based-Models–Felzenszwalb-Girshick-McAllester-Ramanan.pdf 2. Bags of Features(Global + Local Features): “Pedestrian Detection in Crowded Scenes” B. Leibe; E. Seemann; B. Schiele, Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005 http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=1467359 3. R-CNN(Region proposals + CNN) : “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation” R. Girshick, J. Donahue, T. Darrell, J. Malik, Proc. CVPR 2014 https://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf Faster R-CNN: “Towards Real-Time Object Detection with Region Proposal Networks” Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Proc. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016 http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=7485869 4. Multi-filter + Motion, CSS: “New Features and Insights for Pedestrian Detection” S. Walk, N. Majer, K. Schindler, and B. Schiele, Proc. IEEE Computer Vision and Pattern Recognition, 2010. http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=5540102 5. CNN: “Hierarchical Convolutional Features for Visual Tracking” Chao Ma; Jia-Bin Huang; Xiaokang Yang; Ming-Hsuan Yang, Proc. 2015 IEEE International Conference on Computer Vision, 2015 http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=7410709 Part III: Data Set1. Caltech Pedestrian data set P. Dolla´r, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection: A Benchmark” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009. http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/ 2. INRIA N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2005. http://pascal.inrialpes.fr/data/human/ 3. VOC-DPM P. Felzenszwalb, D. McAllester, D. Ramanan, “A Discriminatively Trained, Multiscale, Deformable Part Model” Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2008 https://people.eecs.berkeley.edu/~rbg/latent/ 4. Visual Tracker BenchmarkYi Wu; Jongwoo Lim; Ming-Hsuan Yang “Online Object Tracking: A Benchmark” Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2013 https://sites.google.com/site/trackerbenchmark/benchmarks/v10]]></content>
      <tags>
        <tag>Object Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Recognition Review]]></title>
    <url>%2FObject-Recognition-Review%2F</url>
    <content type="text"><![CDATA[I. The history of recognition II. The process of object recognition http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture17_intro_objrecog_cs131.pdf Step 1. Image features Step 2. Learning A. Classification There are many methods to choose from: • K-nearest neighbor • SVM • Neural networks • Naïve Bayes • Bayesian network • Logic regression • Randomized Forests • Boosted Decision Trees • RBMs • Etc. III. Typical MethodA. Bags of Features http://www.cs.cornell.edu/courses/cs4670/2015sp/lectures/lec35_reco3_web.pdf (1) Origin Texture recognition: • Texture is characterized by the repetition of basic elements or textons • For stochastic textures, the identity of the textons, not their spatial arrangement, mattersJulesz, 1981; Cula &amp; Dana, 2001; Leung &amp; Malik 2001; Mori, Belongie &amp; Malik, 2001; Schmid 2001; Varma &amp; Zisserman, 2002, 2003; Lazebnik, Schmid &amp; Ponce, 2003 Bag-of-words models: Orderless document representation: frequencies of words from a dictionary. Salton &amp; McGill (1983) (2) Outline Step 2: Using K-means clustering Step 4: K nearest neighbors B. CNN(Convolutional Neural Network) http://cs231n.github.io/convolutional-networks/#overview IV. Data SetA. Pedestrian http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=5975165&amp;tag=1 Caltech Pedestrian data setP. Dolla´r, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection: A Benchmark,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009. ETH:A. Ess, B. Leibe, and L. Van Gool, “Depth and Appearance for Mobile Scene Analysis,” Proc. IEEE Int’l Conf. Computer Vision, 2007. TUB-BrusselsC. Wojek, S. Walk, and B. Schiele, “Multi-Cue Onboard Pedestrian Detection,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009 DaimlerM. Enzweiler and D.M. Gavrila, “Monocular Pedestrian Detection: Survey and Experiments,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 12, pp. 2179- 2195, Dec. 2009. INRIAN. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2005. B. Vehicle]]></content>
      <tags>
        <tag>Object Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parsing information from Google Street View image]]></title>
    <url>%2FParsing-information-from-Google-Street-View-image%2F</url>
    <content type="text"><![CDATA[Reference Paper: http://vision.ucsd.edu/sites/default/files/sean_cities.pdf Data Sets Build Up1. Data Collection: The difficulty with Flickr and other consumer photo-sharing websites for geographical tasks is that there is a strong data bias towards famous landmarks. To correct for this bias and provide a more uniform sampling of the geographical space, we turn to GOOGLE STREET VIEW – a huge database of street-level imagery, captured as panoramas using specially-designed vehicles. This enables extraction of roughly fronto-parallel views of building facades and, to some extent, avoids dealing with large variations of camera viewpoint. Google Street View data sets building: http://cmp.felk.cvut.cz/ftp/articles/gronat/Gronat-TR-2011-16.pdfUse panoramas image to reconstruct the city. 2. Data Classification Method: a. Main Idea: We propose an approach that avoids partitioning the entire feature space into clusters. Instead, we start with a large number of randomly sampled candidate patches(Seed of the cluster), and then give each candidate a chance to see if it can converge to a cluster that is both frequent(frequently occurring within the given locale) and discriminative(geographically discriminative, doesn’t occur much in other city), which is labeled as positive. We first compute the nearest neighbors of each candidate, and reject candidates with too many neighbors in the negative set. Then we gradually build clusters by applying iterative discriminative learning(SVM Learning) to each surviving candidate:http://graphics.cs.cmu.edu/projects/whatMakesParis/paris_sigg_reduced.pdf First, the initial geo-informativeness of each patch(a large number of randomly sampled candidate patches) is estimated by finding the top 20 nearest neighbor (NN) patches in the full dataset (both positive and negative), measured by normalized correlation. Patches portraying non-discriminative elements tend to match similar elements in both positive and negative set, while patches portraying a non-repeating element will have more-or-less random matches, also in both sets. Thus, we keep the candidate patches that have the highest proportion of their nearest neighbors in the positive set, while also rejecting near-duplicate patches (measured by spatial overlap of more than 30% between any 5 of their top 50 nearest neighbors). This reduces the number of candidates to about 1000.Then iterative the SVM learning(Figure3 row2 - row4). b. Implementation Details: The implementation considers only square patches (although it would not be difficult to add other aspect ratios), and takes patches at scales ranging from 80-by-80 pixels all the way to height-of-image size. Patches are represented with standard HOG [Dalal and Triggs 2005] (8x8x31 cells), plus a 8x8 color image in Lab colorspace (a and b only). Thus the resulting feature has 8x8x33 = 2112 dimentions. During iterative learning, we use a soft-margin SVM with C fixed to 0.1. The full mining computation is quite expensive; a single city requires approximately 1, 800 CPU-hours. But since the algorithm is highly parallelizable, it can be done overnight on a cluster. c. Reference: Calculate the HOG+color descriptor then use SVM to train the model:https://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf HOG Learning: http://blog.sina.com.cn/s/blog_60e6e3d50101bkpn.html SVM Learning: http://blog.pluskid.org/?page_id=683]]></content>
      <tags>
        <tag>Google Street View</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HOG Learning]]></title>
    <url>%2FHOG-Learning%2F</url>
    <content type="text"><![CDATA[1.Referencehttps://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf 2.Learninghttp://blog.sina.com.cn/s/blog_60e6e3d50101bkpn.html???Algorithm implementation: a.Gradient computation The first step of calculation in many feature detectors in image pre-processing is to ensure normalized color and gamma values. As Dalal and Triggs point out, however, this step can be omitted in HOG descriptor computation, as the ensuing descriptor normalization essentially achieves the same result. Image pre-processing thus provides little impact on performance. Instead, the first step of calculation is the computation of the gradient values. The most common method is to apply the 1-D centered, point discrete?derivative mask?in one or both of the horizontal and vertical directions. Specifically, this method requires filtering the color or intensity data of the image with the following filter kernels: ?? Dalal and Triggs tested other, more complex masks, such as the 3x3?Sobel mask?or diagonal masks, but these masks generally performed more poorly in detecting humans in images. They also experimented with?Gaussian smoothing?before applying the derivative mask, but similarly found that omission of any smoothing performed better in practice. b.Orientation binning The second step of calculation is creating the cell histograms. Each pixel within the cell casts a weighted vote for an orientation-based histogram channel based on the values found in the gradient computation. The cells themselves can either be rectangular or radial in shape, and the histogram channels are evenly spread over 0 to 180 degrees or 0 to 360 degrees, depending on whether the gradient is “unsigned” or “signed”. Dalal and Triggs found that unsigned gradients used in conjunction with 9 histogram channels performed best in their human detection experiments. As for the vote weight, pixel contribution can either be the gradient magnitude itself, or some function of the magnitude. In tests, the gradient magnitude itself generally produces the best results. Other options for the vote weight could include the square root or square of the gradient magnitude, or some clipped version of the magnitude. c.Descriptor blocks To account for changes in illumination and contrast, the gradient strengths must be locally normalized, which requires grouping the cells together into larger, spatially connected blocks. The HOG descriptor is then the concatenated vector of the components of the normalized cell histograms from all of the block regions. These blocks typically overlap, meaning that each cell contributes more than once to the final descriptor. Two main block geometries exist: rectangular R-HOG blocks and circular C-HOG blocks. R-HOG blocks are generally square grids, represented by three parameters: the number of cells per block, the number of pixels per cell, and the number of channels per cell histogram. In the Dalal and Triggs human detection experiment, the optimal parameters were found to be four 8x8 pixels cells per block (16x16 pixels per block) with 9 histogram channels. Moreover, they found that some minor improvement in performance could be gained by applying a Gaussian spatial window within each block before tabulating histogram votes in order to weight pixels around the edge of the blocks less. The R-HOG blocks appear quite similar to the?scale-invariant feature transform?(SIFT) descriptors; however, despite their similar formation, R-HOG blocks are computed in dense grids at some single scale without orientation alignment, whereas SIFT descriptors are usually computed at sparse, scale-invariant key image points and are rotated to align orientation. In addition, the R-HOG blocks are used in conjunction to encode spatial form information, while SIFT descriptors are used singly. ?? Nine Channel from 0-180 degree ?? Blocks are overlap? Circular HOG blocks (C-HOG) can be found in two variants: those with a single, central cell and those with an angularly divided central cell. In addition, these C-HOG blocks can be described with four parameters: the number of angular and radial bins, the radius of the center bin, and the expansion factor for the radius of additional radial bins. Dalal and Triggs found that the two main variants provided equal performance, and that two radial bins with four angular bins, a center radius of 4 pixels, and an expansion factor of 2 provided the best performance in their experimentation(to achieve a good performance, at last use this configure). Also, Gaussian weighting provided no benefit when used in conjunction with the C-HOG blocks. C-HOG blocks appear similar to?shape context?descriptors, but differ strongly in that C-HOG blocks contain cells with several orientation channels, while shape contexts only make use of a single edge presence count in their formulation. d.Block normalization Dalal and Triggs explored four different methods for block normalization. Let v be the non-normalized vector containing all histograms in a given block, ||v||_indexOfk?be its?k-norm for k={1,2}and e be some small constant (the exact value, hopefully, is unimportant). Then the normalization factor can be one of the following: ?? In addition, the scheme L2-hys can be computed by first taking the L2-norm, clipping the result, and then renormalizing. In their experiments, Dalal and Triggs found the L2-hys, L2-norm, and L1-sqrt schemes provide similar performance, while the L1-norm provides slightly less reliable performance; however, all four methods showed very significant improvement over the non-normalized data. e.SVM classifier The final step in object recognition using histogram of oriented gradient descriptors is to feed the descriptors into some recognition system based on supervised learning. The?support vector machine?(SVM) classifier is a binary classifier which looks for an optimal hyperplane as a decision function. Once trained on images containing some particular object, the SVM classifier can make decisions regarding the presence of an object, such as a human, in additional test images. f.Neural Network Classifier The feature of the gradient descriptors are also fed into the neural network classifiers which provides more accuracy in the classification comparing other classifiers (SVM). The neural classifiers can accept the descriptor feature as the binary function or the optimal function. 3. Flow Chart:]]></content>
      <categories>
        <category>Basic Knowledge</category>
      </categories>
      <tags>
        <tag>HOG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>