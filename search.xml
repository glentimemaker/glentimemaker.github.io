<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[RCNN serials Learning Note]]></title>
    <url>%2FRCNN-serials-Learning-Note%2F</url>
    <content type="text"><![CDATA[R-CNNPre-knowledge1. Selective SearchGoal:Propose possible regions of the object. Considerations: 尽可能遍历所有的尺度，但是不同于暴力穷举，可以先得到小尺度的区域，然后一次次合并得到大的尺寸，这样也符合人类的视觉认知。 既然特征很多，那就把我们知道的特征都用上，但是同时也要照顾下计算复杂度，不然和穷举法也没啥区别了。 texture, edge, color Hierarchical Grouping. 能够对每个区域进行排序，这样你想要多少个候选我就产生多少个. Methods a. Algorithm 计算所有相邻区域之间的相似度（相似度函数之后会重点分析），放入集合 S 中，集合 S 保存的其实是一个区域对以及它们之间的相似度。 找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空（需要注意的是，为了计算方便，当两个region合并的时候，新的region的特征应当从之前的两个region的特征直接计算出，而不是再回到图片像素来计算）。 并且，其中每次产生的图像块包括合并的图像块我们都保存下来，这样就得到图像的分层表示。 b. Similarity Calculation 相似度度量公式分为四个子公式，称为互补相似度测量(Complementary Similarity Measures) 。这四个子公式的值都被归一化到区间 [0, 1] 内。 颜色相似度Scolor (ri,rj) 纹理相似度stexture (ri,rj) 尺寸相似度ssize (ri,rj) 填充相似度sfill(ri,rj) c. Combining Locations 我们将覆盖整个区域的 region 的序号标记为 1，合成这个区域的两个子区域的序号为 2，以此类推。但如果仅按序号排序，会存在一个漏洞，那就是区域面积大的会排在前面，为了避免这个漏洞，作者又在每个序号前乘上一个随机数 RND∈[0,1]RND∈[0,1]，通过这个新计算出来的数值，按从小到大的顺序得出 region 最终的排序结果。 2. Non-maximum suppression 非极大值抑制同一个目标会被多个建议框包围，这时需要非极大值抑制操作去除得分较低的候选框以减少重叠框。 (2000×20维矩阵表示每个建议框是某个物体类别的得分情况) ① 对2000×20维矩阵中每列按从大到小进行排序； ② 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值 （IoU: (A∩B)/(A∪B)），则剔除得分较小的建议框，否则认为图像中存在多个同一类物体； ③ 从每列次大的得分建议框开始，重复步骤②； ④ 重复步骤③直到遍历完该列所有建议框； ⑤ 遍历完2000×20维矩阵所有列，即所有物体种类都做一遍非极大值抑制； ⑥ 最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。 Main Steps of RCNN 1. 输入一张多目标图像，采用selective search算法提取约2000个建议框； 2. 先在每个建议框周围加上16个像素值为建议框像素平均值的边框，再直接变形为227×227的大小； (AlexNet pre-trained and fine-tuning:) 3. 先将所有建议框像素减去该建议框像素平均值后【预处理操作】，再依次将每个227×227的建议框输入AlexNet CNN网络获取4096维的特征【比以前的人工经验特征低两个数量级】，2000个建议框的CNN特征组合成2000×4096维矩阵； (SVM training:) 4. 将2000×4096维特征与20个SVM组成的权值矩阵4096×20相乘【20种分类，SVM是二分类器，则有20个SVM】，获得2000×20维矩阵表示每个建议框是某个物体类别的得分。 (Bounding box training:) 5. 分别对上述2000×20维矩阵中每一列即每一类进行非极大值抑制剔除重叠建议框，得到该列即该类中得分最高的一些建议框。 6. 分别用20个回归器对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bounding box。 (用回归器是因为目标检测不仅是要对目标进行识别，还要完成定位任务，所以最终获得的bounding-box也决定了目标检测的精度。对建议框进行校正，使得校正后的Region Proposal与ground truth更接近， 以提高最终的检测精度。) Training Processrcnn首先需要在AlexNet上进行分类的训练model，得到AlexNet之后才能进行分类(Pretrained procedure-&gt;SoftMax2SVM)。分类之后在改一下AxlexNet model （fc: 1000-&gt;21）得到detection model（training）-&gt;(testing)然后在上面利用SVM进行二分类判断当前的region有没有包含我们需要的物体(对结果进行排序，取前面的IOU最大的那几个(nms)),在对这些进行canny边缘检测，才可以得到bounding-box(then B-BoxRegression)。 AlexNet Training1. Pre-training ILSVRC样本集上用于图片分类的含标注类别的训练集有1millon之多，总共含有1000类；而PASCAL VOC 2007样本集上用于物体检测的含标注类别和位置信息的训练集只有10k，总共含有20类，直接用这部分数据训练容易造成过拟合，因此文中利用ILSVRC2012的训练集先进行有监督预训练。 ILSVRC样本集上仅有图像类别标签，没有图像物体位置标注；采用AlexNet CNN网络进行有监督预训练，学习率=0.01；该网络输入为227×227的ILSVRC训练集图像，输出最后一层为4096维特征-&gt;1000类的映射，训练的是网络参数。 2. 特定样本下的微调 PASCAL VOC 2007样本集上既有图像中物体类别标签，也有图像中物体位置标签；采用训练好的AlexNet CNN网络进行PASCAL VOC 2007样本集下的微调，学习率=0.001【0.01/10为了在学习新东西时不至于忘记之前的记忆】；mini-batch为32个正样本和96个负样本【由于正样本太少】；该网络输入为建议框【由selective search而来】变形后的227×227的图像，修改了原来的1000为类别输出，改为21维【20类+背景】输出，训练的是网络参数。 SVM training 由于SVM是二分类器，需要为每个类别训练单独的SVM；SVM训练时输入正负样本在AlexNet CNN网络计算下的4096维特征，输出为该类的得分，训练的是SVM权重向量；由于负样本太多，采用hard negative mining的方法在负样本中选取有代表性的负样本 Bounding box regression training 输入数据为某类型样本对N个：{(Pi,Gi)}i=1⋯N以及Pii=1⋯N所对应的AlexNet CNN网络Pool5层特征ϕ5(Pi)i=1⋯N，输出回归后的建议框Bounding-box，训练的是dx(P)，dy(P)，dw(P)，dh(P)四种变换操作的权重向量。 Fast RCNNR-CNN为什么检测速度这么慢，一张图都需要47s！仔细看下R-CNN框架发现，对图像提完Region Proposal（2000个左右）之后将每个Proposal当成一张图像进行后续处理(CNN提特征+SVM分类)，实际上对一张图像进行了2000次提特征和分类的过程！这2000个Region Proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将Region Proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个Region Proposal的卷积层特征输入到全连接层做后续操作.（对于CNN来说，大部分运算都耗在卷积操作上，这样做可以节省大量时间）. 现在的问题是每个Region Proposal的尺度不一样，直接这样输入全连接层肯定是不行的，因为全连接层输入必须是固定的长度.SPP-NET恰好可以解决这个问题. Main Steps(1)输入测试图像； (2)利用selective search 算法在图像中从上到下提取2000个左右的建议窗口(Region Proposal)； (3)将整张图片输入CNN，进行特征提取； (4)把建议窗口映射到CNN的最后一层卷积feature map上； (5)通过RoI pooling层使每个建议窗口生成固定尺寸的feature map； (6)利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练. Faster RCNNFASTER-RCNN创造性地采用卷积网络自行产生建议框，并且和目标检测网络共享卷积网络，使得建议框数目从原有的约2000个减少为300个，且建议框的质量也有本质的提高. Main Steps(1)输入测试图像； (2)将整张图片输入CNN，进行特征提取； (3)用RPN生成建议窗口(proposals)，每张图片生成300个建议窗口； (4)把建议窗口映射到CNN的最后一层卷积feature map上； (5)通过RoI pooling层使每个RoI生成固定尺寸的feature map； (6)利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练. ReferenceReference 1 Reference 2 Reference 3 Pre-Knowledge]]></content>
      <categories>
        <category>Learning Note</category>
      </categories>
      <tags>
        <tag>RCNN, Fast RCNN, Faster RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reading Note]]></title>
    <url>%2FReading-Note%2F</url>
    <content type="text"><![CDATA[Lifelong Machine LearningIntroductionDefinition Key Characteristics Continuous learning process Knowledge accumulation in KB Use of past knowledge to help future learning Components of LML1. Knowledge Base (KB) Past Information Store (PIS) Knowledge Store (KS) Knowledge Miner (KM) Knowledge Reasoner (KR) 2. Knowledge-Based Learner (KBL) Other Related Learning Methods and Differences with LMLTransfer Learning—Finding correspondences(Pivot features, non-pivot features): Transfer learning typicallyuses labeled training data from one (or more)source domain(s) to help learning in the target domainthat has little or no labeled data (Aue andGamon, 2005, Bollegala et al., 2011). It does notuse the results of the past learning or knowledgemined from the results of the past learning. Further,transfer learning is usually inferior to traditionalsupervised learning when the target domainalready has good training data. Multi-task Learning—Each task model is a linear combination of shared latent components, All task models share latent basic model components Online Learning— The Differences of methods above with LML the Online learning: Lifelong Supervised LearningEarly Works1. Memory based + Distance calculate predicting2. Neural Network3. Task ClusteringNowadays Works1. ELLA: based on GO-MTLN: all tasks (individual)L: all tasks learned from N (latent) 2. Lifelong Sentiment ClassificationComponents i. A classifier for sentiment classification: Use BN and stochastic gradient descent. ii. Embed the past knowledge by using penalty terms so that optimizing the method. iii. Also, consider about dependent classes. 3. Cumulative LearningGoal: Detecting unseen classes in testing. Steps: i. Add new classes ii. Self Learning Semi-supervised never-ending LearningLifelong unsupervised learningLifelong reinforcement learning]]></content>
      <categories>
        <category>Learning Note</category>
      </categories>
      <tags>
        <tag>Life Long Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install and Use Vatic Annotation Tool]]></title>
    <url>%2FInstall-and-Use-Vatic-Annotation-Tool%2F</url>
    <content type="text"><![CDATA[A. How to install Vatic in one line codeVatic is a very convinient online annotation tool for labeling objects of videos. The vatic annotation tool enables users to easily annotate video files, where bounding boxes are automatically interpolated in both size and position. Annotation is performed in a web browser, and annotations are stored in a MySQL database. For this, videos need to be locally published using command line interfacing. Extracting the annotation is also performed using the command line interface. The installation requires a lot of configurations, and it is super painful. The installation steps are shown in Vatic . Instead of installing it step by step manually and encountering lots of bugs, we can easily use other people’s docker environment and run it by VM. 1. Install dockerStep1: undate inner core: sudo apt-get install -y --install-recommends linux-generic-lts-trusty Step2: Use scripts to install the docker curl -sSL https://get.docker.com/ | sh 2. Install and use Vaticreference: vatic-docker Step1: Create “data” folder in the directory where you want to run. Step2: Create “labels.txt” file in data folder. Put all the object types that you want to label on the first line separated by spaces. Step3: Create “videos_in” folder in data folder, and put in the video you want to label. Step4: Run the vatic in docker: sudo docker run -it -p 8111:80 -v $PWD/data:/root/vatic/data npsvisionlab/vatic-docker /bin/bash -C /root/vatic/example.sh 8111 is the port. We can change it when needed. Step5: Annotation Open the browser, type: &lt;Your IP address&gt;:8111/directory (Show ip address: ifconfig) Label the objects in the video, go back to /directory whenever you finish the annotations and click output labels button. Find the output file in the root directory. Docker Commands Show all containers: sudo docker ps -a Kill the running container: sudo docker kill &lt;Container id&gt; Remove the container: sudo docker rm &lt;Container id&gt; B. Another installation methodThe previous method is quite easy and all set-up commands are wrapped in one line code. However, what if you want to do some modifications? For example, you want to modify the length of each segment for the video. Have a look of the Original Vatic Code , you can see that we can modify it by using the --length option while publishing the video. We can’t use those options in the first installation method. So, here is another method, which is a step-by-step installation for vatic in docker. Reference 1. Install Docker (the same as method #1)2. Create folder and LinkCreate one folder named “data” in your current location. Put the video inside, and create another folder named “frames” inside the folder “data”. Then, save the address as a value “DATA_DIR”. DATA_DIR=`pwd`/data/ mkdir -p $DATA_DIR 3. Start the Vatic serverdocker run -it -p 8080:80 -v $DATA_DIR:/home/vagrant/vagrant_data jldowns/vatic-docker-contrib:0.1 4. Start the MySQL and Apache/home/vagrant/start_services.sh 5. Extract the frames from videocd /home/vagrant/vatic turkic extract /home/vagrant/vagrant_data/your_video_name.mp4 /home/vagrant/vagrant_data/frames/ Here, we can use different options to extract the frames, such as modify the ratio by adding --width 1000 --height 800 at the end of above code. More options, see Vatic. 6. Load and Publishturkic load job_id /home/vagrant/vagrant_data/frames/ car skateboard airplane --offline Here, car skateboard airplane are different labels. And in order to modify the length of each segment for the video as we mentioned at the begin. We can use --length option. Just like, turkic load job_id /home/vagrant/vagrant_data/frames/ pedestrain sedan truck van bicycle --offline --length 4000 More options, Vatic. 7. Check all the URLSturkic publish --offline or open the browser and type in http://localhost/?id=1&amp;hitId=offline http://localhost/?id=2&amp;hitId=offline http://localhost/?id=3&amp;hitId=offline ... 8. Save the annotationturkic dump job_id -o /home/vagrant/vagrant_data/annotations.txt The command can also output to many different formats. Available formats are: --xml Use XML --json Use JSON --matlab Use MATLAB --pickle Use Python&apos;s Pickle --labelme Use LabelMe video&apos;s XML format --pascal Use PASCAL VOC format, treating each frame as an image]]></content>
      <categories>
        <category>Data Sets</category>
      </categories>
      <tags>
        <tag>vatic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe Installation in Ubuntu 14.04]]></title>
    <url>%2FCaffe-Installation-in-Ubuntu-14-04%2F</url>
    <content type="text"><![CDATA[PreinstallInstall CUDAInstall cuda by run fileRestart the computer! Install by deb file Install cuDNNStep 1. Download cuDNN from Nvidia cuDNN Step 2. Unzip the package. tar -xzvf &lt;cudnn-tar-file&gt;.tgz Step 3. Copy the *.h header files into cuda. cd ~/Downloads/cuda/include // Copy into the installation path of cuda sudo cp *.h /usr/local/cuda-8.0/include/ Step 4. Copy the library files of cuDNN to the cuda directory cd ~/Downloads/cuda/lib64 sudo cp libcudnn* /usr/local/cuda-8.0/lib64/ Step 5. Set the modify type in the terminal. sudo chmod a+r /usr/local/cuda-8.0/include/cudnn.h /usr/local/cuda-8.0/lib64/libcudnn* Install OpenCVRemove previous installed OpenCVsudo rm -rf /usr/local/include/opencv2 /usr/local/include/opencv /usr/include/opencv /usr/include/opencv2 /usr/local/share/opencv /usr/local/share/OpenCV /usr/share/opencv /usr/share/OpenCV /usr/local/bin/opencv* /usr/local/lib/libopencv* sudo apt-get purge libopencv-* python-data python-opencv sudo apt-get –purge remove opencv-doc opencv-data python-opencv InstallFirst install it’s dependencies: sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev Then download the latest stable version of OpenCV from Sourceforge. Now create a temporary directory ~/opencv and extract the zip file you downloaded into it: mkdir ~/opencv; cd ~/opencv unzip &lt;open-cv-file&gt;.zip -d . Finally, we’ll make the file in another directory we create called release: cd opencv-&lt;version&gt; mkdir release; cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D WITH_IPP=OFF -D CMAKE_INSTALL_PREFIX=/usr/local .. make sudo make install TestCompile samples: $ cd ~/opencv-3.1.0/samples $ cmake . $ sudo make -j $(nproc) Run samples: $ cd ~/opencv-3.1.0/samples/cpp $ ./cpp-example-facedetect ~/Lenna.png Install Caffe]]></content>
      <categories>
        <category>Installation</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN Example#1 Recognize Object]]></title>
    <url>%2FCNN-Example-1-Recognize-Object%2F</url>
    <content type="text"><![CDATA[This article gives a high level overview for the process of recognizing the object by neural network. Step 1To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases: Step 2Let’s combine our four attempts to guess into one big diagram: Step 3Back propagation, using gradient descent algorithm which takes consider of the Cost Function. Step 4Convolution (Make the result translation invariant) 1. Break the image into overlapping image tiles Why we split the image into tiles? Because: Fully connected NN: 1000 × 1000 × 10^6 = 10^12Locally connected NN: 10 × 10 × 10^6 = 10^8 2. Feed each image tile into a small neural network We’ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally(Share the weights). If something interesting appears in any given tile, we’ll mark that tile as interesting. Why we share the weight? Convolutional N: 10 x 10 x 100= 10k 3. Save the results from each tile into a new array Convolution process: 4. Downsampling Use max pooling method to look at each 2x2 square of the array and keep the biggest number, which means to keep the most interesting bit. 5. Do prediction We can use that small array as input into another neural network, this final neural network will decide if the image is or isn’t a match: TipsWhen solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data. The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize. For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it’s knowledge of sharp edges, the third step might recognize entire birds using it’s knowledge of beaks, etc. Referencehttps://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3 https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721]]></content>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Training MNIST dataset by TensorFlow]]></title>
    <url>%2FTraining-MNIST-dataset-by-TensorFlow%2F</url>
    <content type="text"><![CDATA[In this article, I will introduce MNIST data set and review the process of training the MINIST data set to get the model by using TensorFlow. MNIST Data SetThis database is a large database of handwritten digits that is commonly used for training various image processing systems. This database contains 60,000 training images (mnist.train) and 10,000 testing images (mnist.test) 28x28 pixels in one image, we can use 28x28 = 784 dimensions vector to present this matrix. Mnist.train.xs represents 60000 training images. Mnist.train.ys represents the label of the 60000 image. There’re 10 labels from 0 to 9. Each label is the real number shown in each image. Methodology : Softmax RegressionWe use softmax regression to classify each type of writing. 1. The model of the learning 2. Cost function 3. Training Algorithm Use Gradient Descent algorithm, which is backpropagation algorithm.The backpropagation algorithm looks for the minimum of the error function in weight space using the method of gradient descent. Using TensorFlowStep 1: Initialize &amp; start the model init = tf.initialize_all_variables() sess = tf.Session() sess.run(init) Step 2: Training the model (Optimize the weights) for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) // Stochastic training: Randomly use 100 data to train the model. sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) And the train_step is: train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) Step 3: Evaluate the model correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) //y_ is the correct label accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;)) // The accuracy of the model print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) // Print out the accuracy]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Detection Progress]]></title>
    <url>%2FObject-Detection-Progress%2F</url>
    <content type="text"><![CDATA[Part I Computer Vision Field Leaders：http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649439592&amp;idx=1&amp;sn=fdb687300e4a930fdd08c23d8816bbd8&amp;chksm=82c0d4ecb5b75dfae69dd0a219916ab8533da9a6d02d3c7cfbcc5323579c16033bba7407f2b5&amp;scene=21#wechat_redirect David Marr(1945-1980) : He laid the groundwork of computer vision. &lt;&gt; Three major levels of CV: Express(use math to express the problem), Algorithm(to solve the problem), Implement(can be implemented in CPU, DSP or NN) What is computed in CV: primal sketch, 2 1/2 D sketch, 3D sketch, including texture, 3D vision, motion analysis, surface shape. CV is the “progress” of learning the image, is not the result. The longer you observe the image, the more information you will get. 视觉是受任务驱动的，而任务是时刻在改变之中。 视觉求解不是打一个固定的靶子， 而是打一个运动目标。 King-Sun Fu 傅京孫(1930-1985) Syntactic Pattern Recognition Bottom-up and Top-down Ulf Grenander (1923-2016) Pattern Theory( use math and statistic) Proposed analysis-by-synthesis (let the model to generate a image then tell the different between the generated image and the real-world image, then you will know whether this model is a great model) Part II Time Line1999Scale Invariant Feature Transform(SIFT) (improved in 2004) detector descriptor Based on points describe. Feature Based Descriptor (1995~2010)1. Shape Context 2002 Used in MNIST. 2. HOG 2005 Describe the whole patch. 3. Spin Image 1997-&gt;1999 A descriptor of 3D mesh, used in surface matching. 4. STIP (Space time interest points) 2005; HOF (Histogram of oriented optical flow, 2009); MBH (motion boundary histogram, 2013) Object Recognition 2005~20101. LDA (Latent Dirichlet Allocation) 2003 Unsupervised topic modeling, BoW(bag of visual words) algorithm. 2. SPM (Spatial Pyramid Matching) Use spatial grid to separate the image into patches, then calculate the BoW histogram, then combine them together, thus those encoded vector descriptor will have spatial information. 3. Image Encoding Method based on Bow 2006~2009 Sparse coding, Fisher vector to improve BoW (use image encoding) 4. PMK (pyramid matching kernel) 5. DPM (deformable parts models) 2010 Deep Learning 2010~2015Doesn’t need the structure information of the object, multiple layers. n* (convolution layer + pooling layer) + several fully connected layers 1. OverFeat Step 1: Use slide window to get multi-scales ROI. Classify each region by CNN. Step 2: Use regression model to estimate the location of the object. Use bounding box to box the object. Combine the bounding boxes.]]></content>
      <tags>
        <tag>Object Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Detection Methods]]></title>
    <url>%2FObject-Detection-Methods%2F</url>
    <content type="text"><![CDATA[Part I: Basic Method “Histograms of Oriented Gradients for Human Detection,” N. Dalal and W. Triggs, Proc. IEEE CVPR 2005. Improvement: Detect the boundary of the object as well: Learning to Detect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues,” D.R. Martin, C.C. Fowlkes, and J. Malik, Proc. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004. Part II: Object Detection Method1. DPM : “Object Detection Using Discriminatively Trained Part-based Models,” P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, Proc. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. https://people.eecs.berkeley.edu/~rbg/papers/Object-Detection-with-Discriminatively-Trained-Part-Based-Models–Felzenszwalb-Girshick-McAllester-Ramanan.pdf 2. Bags of Features(Global + Local Features): “Pedestrian Detection in Crowded Scenes” B. Leibe; E. Seemann; B. Schiele, Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005 http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=1467359 3. R-CNN(Region proposals + CNN) : “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation” R. Girshick, J. Donahue, T. Darrell, J. Malik, Proc. CVPR 2014 https://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf Faster R-CNN: “Towards Real-Time Object Detection with Region Proposal Networks” Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Proc. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016 http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=7485869 4. Multi-filter + Motion, CSS: “New Features and Insights for Pedestrian Detection” S. Walk, N. Majer, K. Schindler, and B. Schiele, Proc. IEEE Computer Vision and Pattern Recognition, 2010. http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=5540102 5. CNN: “Hierarchical Convolutional Features for Visual Tracking” Chao Ma; Jia-Bin Huang; Xiaokang Yang; Ming-Hsuan Yang, Proc. 2015 IEEE International Conference on Computer Vision, 2015 http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=7410709 Part III: Data Set1. Caltech Pedestrian data set P. Dolla´r, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection: A Benchmark” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009. http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/ 2. INRIA N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2005. http://pascal.inrialpes.fr/data/human/ 3. VOC-DPM P. Felzenszwalb, D. McAllester, D. Ramanan, “A Discriminatively Trained, Multiscale, Deformable Part Model” Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2008 https://people.eecs.berkeley.edu/~rbg/latent/ 4. Visual Tracker BenchmarkYi Wu; Jongwoo Lim; Ming-Hsuan Yang “Online Object Tracking: A Benchmark” Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2013 https://sites.google.com/site/trackerbenchmark/benchmarks/v10]]></content>
      <tags>
        <tag>Object Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Recognition Review]]></title>
    <url>%2FObject-Recognition-Review%2F</url>
    <content type="text"><![CDATA[I. The history of recognition II. The process of object recognition http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture17_intro_objrecog_cs131.pdf Step 1. Image features Step 2. Learning A. Classification There are many methods to choose from: • K-nearest neighbor • SVM • Neural networks • Naïve Bayes • Bayesian network • Logic regression • Randomized Forests • Boosted Decision Trees • RBMs • Etc. III. Typical MethodA. Bags of Features http://www.cs.cornell.edu/courses/cs4670/2015sp/lectures/lec35_reco3_web.pdf (1) Origin Texture recognition: • Texture is characterized by the repetition of basic elements or textons • For stochastic textures, the identity of the textons, not their spatial arrangement, mattersJulesz, 1981; Cula &amp; Dana, 2001; Leung &amp; Malik 2001; Mori, Belongie &amp; Malik, 2001; Schmid 2001; Varma &amp; Zisserman, 2002, 2003; Lazebnik, Schmid &amp; Ponce, 2003 Bag-of-words models: Orderless document representation: frequencies of words from a dictionary. Salton &amp; McGill (1983) (2) Outline Step 2: Using K-means clustering Step 4: K nearest neighbors B. CNN(Convolutional Neural Network) http://cs231n.github.io/convolutional-networks/#overview IV. Data SetA. Pedestrian http://ieeexplore.ieee.org.libproxy.sdsu.edu/stamp/stamp.jsp?arnumber=5975165&amp;tag=1 Caltech Pedestrian data setP. Dolla´r, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection: A Benchmark,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009. ETH:A. Ess, B. Leibe, and L. Van Gool, “Depth and Appearance for Mobile Scene Analysis,” Proc. IEEE Int’l Conf. Computer Vision, 2007. TUB-BrusselsC. Wojek, S. Walk, and B. Schiele, “Multi-Cue Onboard Pedestrian Detection,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009 DaimlerM. Enzweiler and D.M. Gavrila, “Monocular Pedestrian Detection: Survey and Experiments,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 12, pp. 2179- 2195, Dec. 2009. INRIAN. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2005. B. Vehicle]]></content>
      <tags>
        <tag>Object Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parsing information from Google Street View image]]></title>
    <url>%2FParsing-information-from-Google-Street-View-image%2F</url>
    <content type="text"><![CDATA[Reference Paper: http://vision.ucsd.edu/sites/default/files/sean_cities.pdf Data Sets Build Up1. Data Collection: The difficulty with Flickr and other consumer photo-sharing websites for geographical tasks is that there is a strong data bias towards famous landmarks. To correct for this bias and provide a more uniform sampling of the geographical space, we turn to GOOGLE STREET VIEW – a huge database of street-level imagery, captured as panoramas using specially-designed vehicles. This enables extraction of roughly fronto-parallel views of building facades and, to some extent, avoids dealing with large variations of camera viewpoint. Google Street View data sets building: http://cmp.felk.cvut.cz/ftp/articles/gronat/Gronat-TR-2011-16.pdfUse panoramas image to reconstruct the city. 2. Data Classification Method: a. Main Idea: We propose an approach that avoids partitioning the entire feature space into clusters. Instead, we start with a large number of randomly sampled candidate patches(Seed of the cluster), and then give each candidate a chance to see if it can converge to a cluster that is both frequent(frequently occurring within the given locale) and discriminative(geographically discriminative, doesn’t occur much in other city), which is labeled as positive. We first compute the nearest neighbors of each candidate, and reject candidates with too many neighbors in the negative set. Then we gradually build clusters by applying iterative discriminative learning(SVM Learning) to each surviving candidate:http://graphics.cs.cmu.edu/projects/whatMakesParis/paris_sigg_reduced.pdf First, the initial geo-informativeness of each patch(a large number of randomly sampled candidate patches) is estimated by finding the top 20 nearest neighbor (NN) patches in the full dataset (both positive and negative), measured by normalized correlation. Patches portraying non-discriminative elements tend to match similar elements in both positive and negative set, while patches portraying a non-repeating element will have more-or-less random matches, also in both sets. Thus, we keep the candidate patches that have the highest proportion of their nearest neighbors in the positive set, while also rejecting near-duplicate patches (measured by spatial overlap of more than 30% between any 5 of their top 50 nearest neighbors). This reduces the number of candidates to about 1000.Then iterative the SVM learning(Figure3 row2 - row4). b. Implementation Details: The implementation considers only square patches (although it would not be difficult to add other aspect ratios), and takes patches at scales ranging from 80-by-80 pixels all the way to height-of-image size. Patches are represented with standard HOG [Dalal and Triggs 2005] (8x8x31 cells), plus a 8x8 color image in Lab colorspace (a and b only). Thus the resulting feature has 8x8x33 = 2112 dimentions. During iterative learning, we use a soft-margin SVM with C fixed to 0.1. The full mining computation is quite expensive; a single city requires approximately 1, 800 CPU-hours. But since the algorithm is highly parallelizable, it can be done overnight on a cluster. c. Reference: Calculate the HOG+color descriptor then use SVM to train the model:https://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf HOG Learning: http://blog.sina.com.cn/s/blog_60e6e3d50101bkpn.html SVM Learning: http://blog.pluskid.org/?page_id=683]]></content>
      <tags>
        <tag>Google Street View</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HOG Learning]]></title>
    <url>%2FHOG-Learning%2F</url>
    <content type="text"><![CDATA[1.Referencehttps://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf 2.Learninghttp://blog.sina.com.cn/s/blog_60e6e3d50101bkpn.html???Algorithm implementation: a.Gradient computation The first step of calculation in many feature detectors in image pre-processing is to ensure normalized color and gamma values. As Dalal and Triggs point out, however, this step can be omitted in HOG descriptor computation, as the ensuing descriptor normalization essentially achieves the same result. Image pre-processing thus provides little impact on performance. Instead, the first step of calculation is the computation of the gradient values. The most common method is to apply the 1-D centered, point discrete?derivative mask?in one or both of the horizontal and vertical directions. Specifically, this method requires filtering the color or intensity data of the image with the following filter kernels: ?? Dalal and Triggs tested other, more complex masks, such as the 3x3?Sobel mask?or diagonal masks, but these masks generally performed more poorly in detecting humans in images. They also experimented with?Gaussian smoothing?before applying the derivative mask, but similarly found that omission of any smoothing performed better in practice. b.Orientation binning The second step of calculation is creating the cell histograms. Each pixel within the cell casts a weighted vote for an orientation-based histogram channel based on the values found in the gradient computation. The cells themselves can either be rectangular or radial in shape, and the histogram channels are evenly spread over 0 to 180 degrees or 0 to 360 degrees, depending on whether the gradient is “unsigned” or “signed”. Dalal and Triggs found that unsigned gradients used in conjunction with 9 histogram channels performed best in their human detection experiments. As for the vote weight, pixel contribution can either be the gradient magnitude itself, or some function of the magnitude. In tests, the gradient magnitude itself generally produces the best results. Other options for the vote weight could include the square root or square of the gradient magnitude, or some clipped version of the magnitude. c.Descriptor blocks To account for changes in illumination and contrast, the gradient strengths must be locally normalized, which requires grouping the cells together into larger, spatially connected blocks. The HOG descriptor is then the concatenated vector of the components of the normalized cell histograms from all of the block regions. These blocks typically overlap, meaning that each cell contributes more than once to the final descriptor. Two main block geometries exist: rectangular R-HOG blocks and circular C-HOG blocks. R-HOG blocks are generally square grids, represented by three parameters: the number of cells per block, the number of pixels per cell, and the number of channels per cell histogram. In the Dalal and Triggs human detection experiment, the optimal parameters were found to be four 8x8 pixels cells per block (16x16 pixels per block) with 9 histogram channels. Moreover, they found that some minor improvement in performance could be gained by applying a Gaussian spatial window within each block before tabulating histogram votes in order to weight pixels around the edge of the blocks less. The R-HOG blocks appear quite similar to the?scale-invariant feature transform?(SIFT) descriptors; however, despite their similar formation, R-HOG blocks are computed in dense grids at some single scale without orientation alignment, whereas SIFT descriptors are usually computed at sparse, scale-invariant key image points and are rotated to align orientation. In addition, the R-HOG blocks are used in conjunction to encode spatial form information, while SIFT descriptors are used singly. ?? Nine Channel from 0-180 degree ?? Blocks are overlap? Circular HOG blocks (C-HOG) can be found in two variants: those with a single, central cell and those with an angularly divided central cell. In addition, these C-HOG blocks can be described with four parameters: the number of angular and radial bins, the radius of the center bin, and the expansion factor for the radius of additional radial bins. Dalal and Triggs found that the two main variants provided equal performance, and that two radial bins with four angular bins, a center radius of 4 pixels, and an expansion factor of 2 provided the best performance in their experimentation(to achieve a good performance, at last use this configure). Also, Gaussian weighting provided no benefit when used in conjunction with the C-HOG blocks. C-HOG blocks appear similar to?shape context?descriptors, but differ strongly in that C-HOG blocks contain cells with several orientation channels, while shape contexts only make use of a single edge presence count in their formulation. d.Block normalization Dalal and Triggs explored four different methods for block normalization. Let v be the non-normalized vector containing all histograms in a given block, ||v||_indexOfk?be its?k-norm for k={1,2}and e be some small constant (the exact value, hopefully, is unimportant). Then the normalization factor can be one of the following: ?? In addition, the scheme L2-hys can be computed by first taking the L2-norm, clipping the result, and then renormalizing. In their experiments, Dalal and Triggs found the L2-hys, L2-norm, and L1-sqrt schemes provide similar performance, while the L1-norm provides slightly less reliable performance; however, all four methods showed very significant improvement over the non-normalized data. e.SVM classifier The final step in object recognition using histogram of oriented gradient descriptors is to feed the descriptors into some recognition system based on supervised learning. The?support vector machine?(SVM) classifier is a binary classifier which looks for an optimal hyperplane as a decision function. Once trained on images containing some particular object, the SVM classifier can make decisions regarding the presence of an object, such as a human, in additional test images. f.Neural Network Classifier The feature of the gradient descriptors are also fed into the neural network classifiers which provides more accuracy in the classification comparing other classifiers (SVM). The neural classifiers can accept the descriptor feature as the binary function or the optimal function. 3. Flow Chart:]]></content>
      <categories>
        <category>Basic Knowledge</category>
      </categories>
      <tags>
        <tag>HOG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>